{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e138b3c-6ef8-499a-b70a-896dbd63f138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/edwin.a@diggibyte.com/databricks_assignment/src/question_1/source_to_bronze/util\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d7c57e-e35a-4a48-9afc-3587e7ca86e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, avg, sum as sum_, desc\n",
    "\n",
    "# Define paths\n",
    "volume_path = \"/Volumes/workspace/default/assignment_practice\"\n",
    "silver_path = f\"{volume_path}/dim_employee\"\n",
    "bronze_path = f\"{volume_path}/df_write\"\n",
    "gold_path = f\"{volume_path}/gold/employee\"\n",
    "\n",
    "# Step 1: Read silver data (employee delta table)\n",
    "df = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "# Step 2: Read department and country CSVs from bronze\n",
    "department_df = spark.read.option(\"header\", True).csv(f\"{bronze_path}/Department.csv/\")\n",
    "country_df = spark.read.option(\"header\", True).csv(f\"{bronze_path}/Country.csv/\")\n",
    "\n",
    "# Step 3: Convert column names to snake_case\n",
    "department_df = rename_column_to_snake_case(department_df)\n",
    "country_df = rename_column_to_snake_case(country_df)\n",
    "\n",
    "# Step 4: Perform joins using corrected snake_case column names\n",
    "joined_df = df.join(department_df, df[\"department_i_d\"] == department_df[\"department_i_d\"], \"left\") \\\n",
    "              .join(country_df, df[\"country_i_d\"] == country_df[\"country_code\"], \"left\")\n",
    "\n",
    "# Step 5: Add at_load_date column\n",
    "joined_df = add_at_load_date(joined_df)\n",
    "\n",
    "\n",
    "# 1.salary of each department in descending order.\n",
    "salary_df = joined_df.groupBy(\"department_name\") \\\n",
    "    .agg(sum_(\"salary\").alias(\"total_salary\")) \\\n",
    "    .orderBy(desc(\"total_salary\"))\n",
    "\n",
    "salary_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_path}/fact_employee_salary\")\n",
    "\n",
    "# 2.the number of employees in each department located in each country.\n",
    "emp_count_df = joined_df.groupBy(\"department_name\", \"country_name\") \\\n",
    "    .agg(count(\"*\").alias(\"employee_count\"))\n",
    "\n",
    "emp_count_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_path}/fact_employee_count\")\n",
    "\n",
    "# 3.the department names along with their corresponding country names.\n",
    "dept_country_df = joined_df.select(\"department_name\", \"country_name\").distinct()\n",
    "\n",
    "dept_country_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_path}/fact_dept_country\")\n",
    "\n",
    "# 4.the average age of employees in each department\n",
    "avg_age_df = joined_df.groupBy(\"department_name\") \\\n",
    "    .agg(avg(\"age\").alias(\"average_age\"))\n",
    "\n",
    "avg_age_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_path}/fact_avg_age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0bec6e1-6a0b-4a69-9f09-81c279bb098b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "volume_path = \"/Volumes/workspace/default/assignment_practice/gold/employee\"\n",
    "\n",
    "df1 = spark.read.format(\"delta\").load(f\"{volume_path}/fact_employee_salary\")\n",
    "df1.display()\n",
    "\n",
    "df2 = spark.read.format(\"delta\").load(f\"{volume_path}/fact_employee_count\")\n",
    "df2.display()\n",
    "\n",
    "df3 = spark.read.format(\"delta\").load(f\"{volume_path}/fact_avg_age\")\n",
    "df3.display()\n",
    "\n",
    "df4 = spark.read.format(\"delta\").load(f\"{volume_path}/fact_dept_country\")\n",
    "df4.display()\n",
    "\n",
    "joined_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "employee_silver_to_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
